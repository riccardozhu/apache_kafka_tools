kafka-infrastructure/
│
├── zookeeper/
│   ├── docker-compose.yml
│   ├── jmx-exporter-config.yml
│   └── filebeat.yml
│
├── kafka-broker/
│   ├── docker-compose.yml
│   ├── jmx-exporter-config.yml
│   └── filebeat.yml
│
├── monitoring/
│   ├── docker-compose.yml
│   ├── prometheus/
│   │   └── prometheus.yml
│   └── grafana/
│       └── datasources.yml
│
├── elk/
│   ├── docker-compose.yml
│   ├── logstash/
│   │   └── pipeline.conf
│   └── elasticsearch/
│       └── elasticsearch.yml
│
├── security/
│   └── docker-compose.yml
│
└── backup/
    └── docker-compose.yml

# Contenuto di zookeeper/docker-compose.yml
version: '3'
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    volumes:
      - /var/lib/zookeeper/data:/var/lib/zookeeper/data
      - /var/lib/zookeeper/log:/var/lib/zookeeper/log

  jmx-exporter:
    image: sscaling/jmx-prometheus-exporter
    ports:
      - "9308:9308"
    volumes:
      - ./jmx-exporter-config.yml:/etc/jmx_exporter/config.yml

  filebeat:
    image: docker.elastic.co/beats/filebeat:7.15.0
    volumes:
      - ./filebeat.yml:/usr/share/filebeat/filebeat.yml:ro
      - /var/lib/zookeeper/log:/var/lib/zookeeper/log:ro
    command: filebeat -e -strict.perms=false

# Contenuto di kafka-broker/docker-compose.yml
version: '3'
services:
  kafka:
    image: confluentinc/cp-kafka:latest
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    volumes:
      - /var/lib/kafka/data:/var/lib/kafka/data

  jmx-exporter:
    image: sscaling/jmx-prometheus-exporter
    ports:
      - "9308:9308"
    volumes:
      - ./jmx-exporter-config.yml:/etc/jmx_exporter/config.yml

  filebeat:
    image: docker.elastic.co/beats/filebeat:7.15.0
    volumes:
      - ./filebeat.yml:/usr/share/filebeat/filebeat.yml:ro
      - /var/lib/kafka/logs:/var/lib/kafka/logs:ro
    command: filebeat -e -strict.perms=false

# Contenuto di monitoring/docker-compose.yml
version: '3'
services:
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    volumes:
      - ./grafana/datasources.yml:/etc/grafana/provisioning/datasources/datasources.yml
      - grafana_data:/var/lib/grafana

volumes:
  prometheus_data:
  grafana_data:

# Contenuto di elk/docker-compose.yml
version: '3'
services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.15.0
    ports:
      - "9200:9200"
      - "9300:9300"
    environment:
      - discovery.type=single-node
    volumes:
      - ./elasticsearch/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml
      - elasticsearch_data:/usr/share/elasticsearch/data

  logstash:
    image: docker.elastic.co/logstash/logstash:7.15.0
    ports:
      - "5044:5044"
    volumes:
      - ./logstash/pipeline.conf:/usr/share/logstash/pipeline/logstash.conf
    depends_on:
      - elasticsearch

  kibana:
    image: docker.elastic.co/kibana/kibana:7.15.0
    ports:
      - "5601:5601"
    environment:
      ELASTICSEARCH_URL: http://elasticsearch:9200
    depends_on:
      - elasticsearch

volumes:
  elasticsearch_data:

# Contenuto di security/docker-compose.yml
version: '3'
services:
  vault:
    image: vault:latest
    ports:
      - "8200:8200"
    environment:
      VAULT_DEV_ROOT_TOKEN_ID: "myroot"
    cap_add:
      - IPC_LOCK

# Contenuto di backup/docker-compose.yml
version: '3'
services:
  backup:
    image: alpine:latest
    volumes:
      - /path/to/backup:/backup
    command: sh -c "echo 'Backup service placeholder'"

# Contenuto di zookeeper/filebeat.yml e kafka-broker/filebeat.yml
filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /var/lib/*/logs/*.log
  fields:
    kafka_topic: log-topic

output.kafka:
  hosts: ["kafka:9092"]
  topic: '%{[fields.kafka_topic]}'

# Contenuto di elk/logstash/pipeline.conf
input {
  kafka {
    bootstrap_servers => "kafka:9092"
    topics => ["log-topic"]
    codec => json
  }
}

filter {
  grok {
    match => { "message" => "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:log_level} %{GREEDYDATA:log_message}" }
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "logs-%{+YYYY.MM.dd}"
  }
}

# Contenuto di elk/elasticsearch/elasticsearch.yml
cluster.name: "docker-cluster"
network.host: 0.0.0.0